---
title: "Balanceo en la GEIH"
author: "Wilson Andrés Pinzon"
output: pdf_document
bibliography: r-references.bib
biblio-style: "apalike"
link-citations: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE, include=FALSE}
## Librerias Necesarias y Archivos Base
library(survey)
library(weights)
library(dplyr)

# Scripts de R con las funciones de balanceo
source("C:\\Users\\andre\\OneDrive\\Escritorio\\Proyecto de Grado\\notebooks\\R\\oversampling.R")

## Dataframe Base
data <- read.csv("C:/Users/andre/OneDrive/Escritorio/Proyecto de Grado/result/db_agrupado/Enero_Agrupado.csv", sep=";")
data <- subset(data, select = -DIRECTORIO)

### Procesamiento Inicial y Necesario
data$ACTIVIDAD_OCUPADA_ULTIMA_SEMANA = as.factor(data$ACTIVIDAD_OCUPADA_ULTIMA_SEMANA)
vars.numeric <- names(data)[sapply(data, is.numeric)]
vars.numeric <- vars.numeric[vars.numeric != "FACTOR_EXPANSION"]

vars.categoric <- names(data)[!names(data) %in% vars.numeric]
vars.categoric <- vars.categoric[vars.categoric != "ACTIVIDAD_OCUPADA_ULTIMA_SEMANA"]

## Plantación de Semilla y Valores Base
seed = 3
set.seed(seed)
size = 0.8
target = "ACTIVIDAD_OCUPADA_ULTIMA_SEMANA"
minority.value = 0
k = 5

## Definición del Conjunto de Prueba y Entrenamiento, ajuste del conjunto de entrenamiento
## Train Test split
train_index <- sample(nrow(data), size = size * nrow(data), replace = FALSE)

train <- data[train_index, ]
test <- data[-train_index, ]

rownames(train) <- NULL
rownames(test) <- NULL

## Ajuste del conjunto de entrenamiento para la generación de las muestras sintéticas
train.nowt <- subset(train, select = -FACTOR_EXPANSION)

train.nowt$ACTIVIDAD_OCUPADA_ULTIMA_SEMANA = as.factor(train.nowt$ACTIVIDAD_OCUPADA_ULTIMA_SEMANA)
vars.numeric <- names(train.nowt)[sapply(train.nowt, is.numeric)]
```

# Balanceo en la GEIH

En el siguiente documento se presenta un método que busca tratar el problema del desbalance respecto a la variable `ACTIVIDAD_OCUPADA_ULTIMA_SEMANA`, la cual será la variable dependiente de un modelo de regresión logística que busca explicar qué impacto tienen algunas condiciones sociales y demográficas en la probabilidad de que un joven de 18 a 28 años que pertenece a la fuerza de trabajo se encuentre trabajando.

Este método consiste en el uso de algún método de balanceo de tipo oversampling sobre la clase minoritaría, como SMOTE o ADASYN, para generar un conjunto de muestras sintéticas. Posteriormente, se utiliza el método *Propensity Score Adjusted (PSA)* para generar unos *pseudopesos* que permitan ponderar el conjunto de muestras sintéticas para realizar una estimación híbrida del modelo de regresión logística que combina los datos del conjunto original y el conjunto de muestras sintéticas.

En el siguiente documento se evaluaran tres variaciones del método SMOTE para tratar el problema de balanceo de datos, los cuales son:

1.  SMOTE - ENC

2.  ADASYN modificado con SMOTE-ENC

3.  SMOTE-ENC ajustado con pesos de muestreo

Posteriormente, se aplicará el método de PSA para generar los pseudopesos para cada conjunto de muestras sintéticas generados, finlamente se obtendran tres estimaciones del modelo de regresión logística donde se evaluara el desempeño obtenido a través de cada método de balanceo, de esta manera se podrá determinar qué variación de SMOTE junto al método de PSA se ajusta mejor al problema de balanceo de datos en la GEIH.

# Balanceo

El desbalance en los datos se refiere al caso en el que el conjunto de datos no tiene un representación equitativa de instancias que pertenecen a cada clase de la variable dependiente, en este caso, se presenta un desbalance de dos clases, en donde hay una gran diferencia en el número total de instancias que pertenecen a una clase respecto a la otra, comunmente se les denomina clase mayoritaría y minoritaria.

Actualmente el total de instancias del conjunto de datos de la GEIH que pertenecen a cada clase de la variable dependiente es la siguiente:

```{r}
table(data$ACTIVIDAD_OCUPADA_ULTIMA_SEMANA)
```

Donde el valor $'0'$ indica que el joven está buscando trabajo mientras que el valor $'1'$ indica que ya se encuentra trabajando. Con esto en cuenta, se puede observar que hay un desbalance en los datos donde la clase minoritaría se refiere a la clase donde los jovenes que están buscando trabajo.

El tema de desbalance de los datos es un tema desafiante en el Machine Learning. Según Mukherjee et.al [-@Mukherjee2021], "los algoritmos de machine learning tienden a predecir cualquier instancia como un elemento de la clase mayoritaria, haciendo que el modelo resulte ineficiente para identificar las instancias de la clase minoritaria, esto es algo crítico, especialmente, cuando hay un gran interes en clasificar de manera correcta esta clase".

Se han desarrollado varías formas para tratar este problema, una de las formas principales consiste en realizar un re-muestreo del conjunto de datos, esto puede ser a través del oversampling de la clase minoritaría o del subsampling de la clase mayoritaría. Entre estos métodos de re-muestreo, uno de los métodos más utilizados es el método SMOTE [@Chawla2002].

SMOTE es un algoritmo en el cual la clase minoritaría recibe un oversampling a través de la creación de muestras "*sintéticas*" que se ubican en los segmentos que unen a cada instancia de la clase minoritatía con sus $k$ vecinos más cercanos en cada variable o característica.

SMOTE ha ganado una gran popularidad entre los métodos que existen para tratar el problema de balanceo y de hecho, se ha establecido como uno de los métodos más utilizados para tratar este problema. Además, desde su desarrollo han salido multiples variantes cómo los con Borderline-SMOTE, ADASYN, SMOTE ENN, entre otros, que buscan mejorar su rendimiento endiferentes escenarios.

Ahora, un problema que tienen estos métodos, es que fueron desarrollados bajo la consideración que todas las variables son continua, en el caso que se trabaje sobre un conjunto de datos que contiene variables nominales, como es el caso de la GEIH, tanto SMOTE como sus variantes no son directamente aplicables. Si bien existe una variante en donde se codifican las variables nominales a través de la técnica One Hot Encoding, esta variante no es la mejor solución ya que aumenta considerablemente el costo computacional del algoritmo y además, es posible que el algoritmo no aprenda sobre las posibles relaciones entre los valores nominales y las clases.

Por este motivo, se han desarrollado variantes de SMOTE que permiten manejar variables nominales y continuas. Una de estas variantes es SMOTE-ENC(SMOTE Encoded Nominal and Continuous) [@Mukherjee2021], en donde las variables nominales son codificadas como variables numéricas y en donde un valor más alto representa una asociación más fuerte con la clase minoritaría.

Este algoritmo será el primer método que se utilizará como método para tratar el problema de balanceo en la GEIH, para esto se generará el conjunto de muestras sintéticas sobre un conjunto de entrenamiento:

```{r}
### Método SMOTE ENC
synt.smote = SMOTE_ENC(train.nowt, target, minority.value, vars.numeric, k , seed)
```

```{r include=FALSE}
synt.smote <- synt.smote %>% mutate(across(all_of(vars.numeric), as.numeric))
```

SMOTE-ENC es una alternativa que nos permite tratar el problema del desbalance en conjuntos con variables numéricas y nominales, sin embargo, es cabe resaltar que este método proviene de SMOTE y por lo tanto, puede heredar algunas de las limitaciones de este método.

En ese sentido, ADASYN (Adaptive Synthetic) [@He2008] surge como una de las variantes de SMOTE más robusta. Este método se basa en la idea de generar muestras sintéticas de la clase minoritaría de forma adaptativa, es decir, busca generar más muestras sintéticas de auqellas intancias con una menor densidad.

La mayor diferencia entre SMOTE y ADASYN radica en que SMOTE genera la misma cantidad de registros sintéticos para cada muestra de la clase minoritaria, mientras que ADASYN provee un peso a cada registro de la clase minoritaria para determinar el número de muestras sintéticas que deben ser generadas por cada registro.

Ahora, como se comento anteriormente, ciertas variantes de SMOTE, incluyendo ADASYN, funcionan bajo la consideración que todas las variables son numéricas, así que, para tratar este problema, se propone realizar una variante del algoritmo ADASYN, donde se planea utilizar la métrica dispuesta en el método SMOTE-ENC para encontrar los $k$ vecinos más cercanos de cada instancia de la clase minoritaria. La utilización de esta métrica permite combinar la robustez de ADASYN con la capacidad que tiene SMOTE-ENC para tratar variables numéricas y nominales.

Esta variante de ADASYN con SMOTE-ENC será el segundo método utilizado dentro del documento para tratar el problema de desbalance de datos en la GEIH, se generará el conjunto de muestras sintéticas sobre el mismo conjunto de entrenamiento utilizado en SMOTE-ENC.

```{r}
### Método ADASYN con SMOTE-ENC
synt.adasyn = ADASYN(train.nowt, target, minority.value, vars.numeric, k , seed)
```

```{r include=FALSE}
synt.adasyn <- synt.adasyn %>% mutate(across(all_of(vars.numeric), as.numeric))
```

Además de los métodos SMOTE-ENC y ADASYN, este documento propone una tercera alternativa para tratar el problema del desbalance de datos, esta alternativa está basado en el algoritmo WSMOTE [@Prusty2017].

WSMOTE es un método de sobre muestreo que asigna unos pesos a cada instancia y que determinan el número de muestras sintéticas que se van a genrar a través de SMOTE para cada instancia de la clase minoritaria.

La variante propuesta consiste en adaptar la idea del uso de unos pesos para determinar el número de muestras sintéticas, sin embargo, en el caso propuesto, se elimina el calculo de los pesos explícitos, en su lugar, se van a utilizar los pesos de muestreo asociados al conjunto de datos para determinar el número de muestras sintéticas que se van a generar de cada instancia. Se propone además, para poder tener un número adecuado de muestras sintéticas, normalizar los pesos de muestreo.

Esta propuesta se basa en la idea de que los pesos de muestreo representan el porcentaje de la población que representa cada registro dentro del conjunto de datos. Utilizar estos pesos normalizados, permite generar un conjunto de muestras sintéticas que procura preservar las distribuciones asociadas a la población original.

Otra consideración es que WSMOTE es una variante que determina la cantidad de muestras sintéticas que se van a generar de cada instancia, pero, genera estas muestras a través de SMOTE, por lo cual, trabaja bajo la consideración que todas las variables son numéricas, en ese sentido, la propuesta de SMOTE con Pesos de muestreo también utilizará la metrica de SMOTE-ENC para encontrar los $k$ vecinos más cercanos.

La generación del conjunto de muestras sintéticas para esta propuesta de método también será realizada a partir del conjunto de entrenamiento:\

```{r}
factor.expansion <- train$FACTOR_EXPANSION
synt.swsmoteenc = SWSMOTEENC(train.nowt,
                             factor.expansion,
                             target, 
                             minority.value,
                             vars.numeric,
                             k ,
                             seed)

```

```{r include=FALSE}
synt.swsmoteenc <- synt.swsmoteenc %>% mutate(across(all_of(vars.numeric), as.numeric))
```

Ahora, un detalle muy importante a tener en cuenta es que el conjunto de datos de la GEIH, es un conjunto de datos basado en un muestreo complejo, por lo que el peso de muestreo tiene un papel fundamental sobre cualquier tipo de análisis que se desee realizar. Este valor permite ajustar los resultados del conjunto de datos para obtener estimaciones más precisas sobre la población de estudio, y de hecho, ignorar esta variable puede llevar a realizar estimaciones imprecisas.

En el contexto del desbalance de datos, este peso de muestreo es un factor fundamental. SMOTE muestra probabilística y no probabilística.

Para resolver este problema, a través del Propensity Score [@EbrahimValojerdi2018], esta idea parte de la formula

$$ P(S^* =1|W) $$

Donde $S$ es un indicador para saber si un elemento de la población pertenece a la muestra probabilística y $S^*$ es un indicador para saber si un elemento de la población pertenece a la muestra no probabilística, a partir de un conjunto de covariables $W$. A través de las estimaciones de la probabilidad es posible obtener unos pseudopesos a través del valor $1/\hat{P}(S^*=1|W)$.

Más aún, si definimos a $Z$ como un indicador para conocer si un elemento pertenece a la muestra probabilística $Z=1$. Entonces podemos tener la probabilidad de $P(Z=1|W)$, y para una muestra lo suficientemente grande vamos a tener:

$$\frac{P(W|S*=1)}{P(W|S=1)} \propto \frac{P(Z=1|W)}{P(Z=0|W)}$$

Así que, para estimar esta probabilidad, se utiliza una regresión logística sobre el conjunto que contiene la combinación de las muestras que permita estimar la probabilidad de $Z$. Los pesos iniciales para las instancias no probabilísticas serán de $1$, mientras que las instancias de la muestra probabilística usaran sus pesos ajustados. Así, la inversa de la probabilidad resultante para las instancias no probabilísticas serán el pseudopeso asocidado.

Es muy importante calibrar o ajustar los pesos obtenidos a los totales de la población, según @Elliott2009, es posible realizar esto a partir de la siguiente formula:

-   Para la muestra no probabilística: $\hat{w_i} = C_{s^*} \times \tilde{w_i}$ donde: $C_{S^*} = \frac{n_{s^*}}{n_{s^*}+n_s} \cdot \frac{\sum_{i\in S} w_i}{\sum_{j\in S^*} \tilde{w_j}}$

-   Para la muestra probabilística $\hat{w_i} = C_{s} \times w_i$ donde: $C_{S} = \frac{n_{s}}{n_{s^*}+n_s}.$

Ahora, antes de realizar este proceso es muy importante tener en cuenta que es necesario que ambas muestras deben cubrir la misma porción de la población, esta es una consideración crucial ya que según [@dever2018combining], en el caso que esto no se cumpla el modelo no necesariamente es consistente.

## Evaluación de los modelos

## Referencias
